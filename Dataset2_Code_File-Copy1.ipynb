{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "188cf974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.layers import Conv2D, Flatten, MaxPooling2D,Dense,Dropout,SpatialDropout2D\n",
    "from keras.models  import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import random,os,glob\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eedc2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'F:/UWL Study Documents/Dissertation/Dataset3/Garbage classification/Garbage classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70031167",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = glob.glob(os.path.join(dir_path, '*/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b5df8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2527"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e7365d",
   "metadata": {},
   "source": [
    "Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "288a171e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2276 images belonging to 6 classes.\n",
      "Found 251 images belonging to 6 classes.\n",
      "{'cardboard': 0, 'glass': 1, 'metal': 2, 'paper': 3, 'plastic': 4, 'trash': 5}\n",
      "{0: 'cardboard', 1: 'glass', 2: 'metal', 3: 'paper', 4: 'plastic', 5: 'trash'}\n"
     ]
    }
   ],
   "source": [
    "train=ImageDataGenerator(horizontal_flip=True,\n",
    "                         vertical_flip=True,\n",
    "                         validation_split=0.1,\n",
    "                         rescale=1./255,\n",
    "                         shear_range = 0.1,\n",
    "                         zoom_range = 0.1,\n",
    "                         width_shift_range = 0.1,\n",
    "                         height_shift_range = 0.1,)\n",
    "\n",
    "test=ImageDataGenerator(rescale=1/255,\n",
    "                        validation_split=0.1)\n",
    "\n",
    "train_generator=train.flow_from_directory(dir_path,\n",
    "                                          target_size=(300,300),\n",
    "                                          batch_size=32,\n",
    "                                          class_mode='categorical',\n",
    "                                          subset='training')\n",
    "\n",
    "test_generator=test.flow_from_directory(dir_path,\n",
    "                                        target_size=(300,300),\n",
    "                                        batch_size=32,\n",
    "                                        class_mode='categorical',\n",
    "                                        subset='validation')\n",
    "\n",
    "labels = (train_generator.class_indices)\n",
    "print(labels)\n",
    "\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51e05fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 300, 300, 3), (32, 6))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for image_batch, label_batch in train_generator:\n",
    "  break\n",
    "image_batch.shape, label_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18282ff",
   "metadata": {},
   "source": [
    "Writing the labels file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ec47fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cardboard': 0, 'glass': 1, 'metal': 2, 'paper': 3, 'plastic': 4, 'trash': 5}\n"
     ]
    }
   ],
   "source": [
    "print (train_generator.class_indices)\n",
    "\n",
    "Labels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n",
    "\n",
    "with open('labels.txt', 'w') as f:\n",
    "  f.write(Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc197185",
   "metadata": {},
   "source": [
    "Building CNN & Saving keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90f919f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "#Convolution blocks\n",
    "\n",
    "model.add(Conv2D(32,(3,3), padding='same',input_shape=(300,300,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2)) \n",
    "#model.add(SpatialDropout2D(0.5)) # No accuracy\n",
    "\n",
    "model.add(Conv2D(64,(3,3), padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2)) \n",
    "#model.add(SpatialDropout2D(0.5))\n",
    "\n",
    "model.add(Conv2D(32,(3,3), padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2)) \n",
    "\n",
    "#Classification layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "#model.add(SpatialDropout2D(0.5))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(6,activation='softmax'))\n",
    "\n",
    "filepath=\"trained_model.h5\"\n",
    "checkpoint1 = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677440b9",
   "metadata": {},
   "source": [
    "Summarizing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e725552b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 300, 300, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 150, 150, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 150, 150, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 75, 75, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 75, 75, 32)        18464     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 37, 37, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 43808)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                2803776   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 198       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2843910 (10.85 MB)\n",
      "Trainable params: 2843910 (10.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9849d07f",
   "metadata": {},
   "source": [
    "Using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c3c0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9736110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorBoard callback\n",
    "tensorboard_callback = TensorBoard(log_dir='F:/UWL Study Documents/Dissertation/Dataset2_Logs', histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6d5ed",
   "metadata": {},
   "source": [
    "Compiling Model using categorical cross entropy loss function & Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceaea56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc']) # RMS PROP - No accuracy\n",
    "\n",
    "#es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f2d88",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a591db72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\123456\\AppData\\Local\\Temp\\ipykernel_10344\\3060681096.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.6262 - acc: 0.3209\n",
      "Epoch 1: val_acc improved from -inf to 0.46429, saving model to trained_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 283s 4s/step - loss: 1.6262 - acc: 0.3209 - val_loss: 1.3926 - val_acc: 0.4643\n",
      "Epoch 2/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.4734 - acc: 0.3988\n",
      "Epoch 2: val_acc did not improve from 0.46429\n",
      "71/71 [==============================] - 265s 4s/step - loss: 1.4734 - acc: 0.3988 - val_loss: 1.4593 - val_acc: 0.4196\n",
      "Epoch 3/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.3728 - acc: 0.4483\n",
      "Epoch 3: val_acc improved from 0.46429 to 0.47321, saving model to trained_model.h5\n",
      "71/71 [==============================] - 269s 4s/step - loss: 1.3728 - acc: 0.4483 - val_loss: 1.3678 - val_acc: 0.4732\n",
      "Epoch 4/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.3356 - acc: 0.4675\n",
      "Epoch 4: val_acc improved from 0.47321 to 0.47768, saving model to trained_model.h5\n",
      "71/71 [==============================] - 263s 4s/step - loss: 1.3356 - acc: 0.4675 - val_loss: 1.3019 - val_acc: 0.4777\n",
      "Epoch 5/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.3076 - acc: 0.4871\n",
      "Epoch 5: val_acc did not improve from 0.47768\n",
      "71/71 [==============================] - 284s 4s/step - loss: 1.3076 - acc: 0.4871 - val_loss: 1.3188 - val_acc: 0.4598\n",
      "Epoch 6/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.3146 - acc: 0.4768\n",
      "Epoch 6: val_acc improved from 0.47768 to 0.48661, saving model to trained_model.h5\n",
      "71/71 [==============================] - 257s 4s/step - loss: 1.3146 - acc: 0.4768 - val_loss: 1.3080 - val_acc: 0.4866\n",
      "Epoch 7/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.2845 - acc: 0.4969\n",
      "Epoch 7: val_acc did not improve from 0.48661\n",
      "71/71 [==============================] - 253s 3s/step - loss: 1.2845 - acc: 0.4969 - val_loss: 1.3232 - val_acc: 0.4643\n",
      "Epoch 8/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.2918 - acc: 0.4889\n",
      "Epoch 8: val_acc did not improve from 0.48661\n",
      "71/71 [==============================] - 258s 4s/step - loss: 1.2918 - acc: 0.4889 - val_loss: 1.4089 - val_acc: 0.4554\n",
      "Epoch 9/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.2394 - acc: 0.5192\n",
      "Epoch 9: val_acc improved from 0.48661 to 0.49554, saving model to trained_model.h5\n",
      "71/71 [==============================] - 250s 3s/step - loss: 1.2394 - acc: 0.5192 - val_loss: 1.2582 - val_acc: 0.4955\n",
      "Epoch 10/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.2319 - acc: 0.5201\n",
      "Epoch 10: val_acc improved from 0.49554 to 0.52232, saving model to trained_model.h5\n",
      "71/71 [==============================] - 363s 5s/step - loss: 1.2319 - acc: 0.5201 - val_loss: 1.2150 - val_acc: 0.5223\n",
      "Epoch 11/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.2131 - acc: 0.5174\n",
      "Epoch 11: val_acc did not improve from 0.52232\n",
      "71/71 [==============================] - 296s 4s/step - loss: 1.2131 - acc: 0.5174 - val_loss: 1.2059 - val_acc: 0.5134\n",
      "Epoch 12/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.1729 - acc: 0.5535\n",
      "Epoch 12: val_acc did not improve from 0.52232\n",
      "71/71 [==============================] - 319s 4s/step - loss: 1.1729 - acc: 0.5535 - val_loss: 1.2794 - val_acc: 0.5089\n",
      "Epoch 13/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.1442 - acc: 0.5668\n",
      "Epoch 13: val_acc improved from 0.52232 to 0.53571, saving model to trained_model.h5\n",
      "71/71 [==============================] - 292s 4s/step - loss: 1.1442 - acc: 0.5668 - val_loss: 1.2139 - val_acc: 0.5357\n",
      "Epoch 14/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.1161 - acc: 0.5829\n",
      "Epoch 14: val_acc improved from 0.53571 to 0.55804, saving model to trained_model.h5\n",
      "71/71 [==============================] - 321s 4s/step - loss: 1.1161 - acc: 0.5829 - val_loss: 1.1673 - val_acc: 0.5580\n",
      "Epoch 15/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.1263 - acc: 0.5731\n",
      "Epoch 15: val_acc did not improve from 0.55804\n",
      "71/71 [==============================] - 297s 4s/step - loss: 1.1263 - acc: 0.5731 - val_loss: 1.1421 - val_acc: 0.5312\n",
      "Epoch 16/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.0730 - acc: 0.5994\n",
      "Epoch 16: val_acc improved from 0.55804 to 0.57143, saving model to trained_model.h5\n",
      "71/71 [==============================] - 250s 3s/step - loss: 1.0730 - acc: 0.5994 - val_loss: 1.1543 - val_acc: 0.5714\n",
      "Epoch 17/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.0613 - acc: 0.6016\n",
      "Epoch 17: val_acc improved from 0.57143 to 0.57589, saving model to trained_model.h5\n",
      "71/71 [==============================] - 251s 3s/step - loss: 1.0613 - acc: 0.6016 - val_loss: 1.0552 - val_acc: 0.5759\n",
      "Epoch 18/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.0254 - acc: 0.6141\n",
      "Epoch 18: val_acc improved from 0.57589 to 0.63839, saving model to trained_model.h5\n",
      "71/71 [==============================] - 308s 4s/step - loss: 1.0254 - acc: 0.6141 - val_loss: 1.0352 - val_acc: 0.6384\n",
      "Epoch 19/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.9977 - acc: 0.6168\n",
      "Epoch 19: val_acc improved from 0.63839 to 0.68304, saving model to trained_model.h5\n",
      "71/71 [==============================] - 263s 4s/step - loss: 0.9977 - acc: 0.6168 - val_loss: 0.9351 - val_acc: 0.6830\n",
      "Epoch 20/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.9584 - acc: 0.6430\n",
      "Epoch 20: val_acc did not improve from 0.68304\n",
      "71/71 [==============================] - 226s 3s/step - loss: 0.9584 - acc: 0.6430 - val_loss: 1.0952 - val_acc: 0.6116\n",
      "Epoch 21/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.9968 - acc: 0.6319\n",
      "Epoch 21: val_acc did not improve from 0.68304\n",
      "71/71 [==============================] - 225s 3s/step - loss: 0.9968 - acc: 0.6319 - val_loss: 1.0336 - val_acc: 0.5804\n",
      "Epoch 22/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 1.0052 - acc: 0.6225\n",
      "Epoch 22: val_acc did not improve from 0.68304\n",
      "71/71 [==============================] - 225s 3s/step - loss: 1.0052 - acc: 0.6225 - val_loss: 0.9812 - val_acc: 0.6473\n",
      "Epoch 23/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.9843 - acc: 0.6399\n",
      "Epoch 23: val_acc did not improve from 0.68304\n",
      "71/71 [==============================] - 225s 3s/step - loss: 0.9843 - acc: 0.6399 - val_loss: 0.9717 - val_acc: 0.6473\n",
      "Epoch 24/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.9618 - acc: 0.6413\n",
      "Epoch 24: val_acc did not improve from 0.68304\n",
      "71/71 [==============================] - 226s 3s/step - loss: 0.9618 - acc: 0.6413 - val_loss: 0.9753 - val_acc: 0.6339\n",
      "Epoch 25/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.9076 - acc: 0.6689\n",
      "Epoch 25: val_acc did not improve from 0.68304\n",
      "71/71 [==============================] - 293s 4s/step - loss: 0.9076 - acc: 0.6689 - val_loss: 1.0344 - val_acc: 0.6116\n",
      "Epoch 26/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.9097 - acc: 0.6725\n",
      "Epoch 26: val_acc did not improve from 0.68304\n",
      "71/71 [==============================] - 273s 4s/step - loss: 0.9097 - acc: 0.6725 - val_loss: 1.0143 - val_acc: 0.6250\n",
      "Epoch 27/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.9335 - acc: 0.6578\n",
      "Epoch 27: val_acc did not improve from 0.68304\n",
      "71/71 [==============================] - 229s 3s/step - loss: 0.9335 - acc: 0.6578 - val_loss: 1.0019 - val_acc: 0.5893\n",
      "Epoch 28/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.8874 - acc: 0.6615\n",
      "Epoch 28: val_acc did not improve from 0.68304\n",
      "71/71 [==============================] - 285s 4s/step - loss: 0.8874 - acc: 0.6615 - val_loss: 0.8916 - val_acc: 0.6473\n",
      "Epoch 29/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.8418 - acc: 0.6872\n",
      "Epoch 29: val_acc did not improve from 0.68304\n",
      "71/71 [==============================] - 291s 4s/step - loss: 0.8418 - acc: 0.6872 - val_loss: 0.9518 - val_acc: 0.6205\n",
      "Epoch 30/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.8411 - acc: 0.6889\n",
      "Epoch 30: val_acc did not improve from 0.68304\n",
      "71/71 [==============================] - 278s 4s/step - loss: 0.8411 - acc: 0.6889 - val_loss: 0.8846 - val_acc: 0.6429\n",
      "Epoch 31/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.8584 - acc: 0.6818\n",
      "Epoch 31: val_acc did not improve from 0.68304\n",
      "71/71 [==============================] - 278s 4s/step - loss: 0.8584 - acc: 0.6818 - val_loss: 0.8464 - val_acc: 0.6830\n",
      "Epoch 32/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.8377 - acc: 0.6921\n",
      "Epoch 32: val_acc did not improve from 0.68304\n",
      "71/71 [==============================] - 270s 4s/step - loss: 0.8377 - acc: 0.6921 - val_loss: 0.9090 - val_acc: 0.6741\n",
      "Epoch 33/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.8283 - acc: 0.7019\n",
      "Epoch 33: val_acc improved from 0.68304 to 0.69196, saving model to trained_model.h5\n",
      "71/71 [==============================] - 285s 4s/step - loss: 0.8283 - acc: 0.7019 - val_loss: 0.8093 - val_acc: 0.6920\n",
      "Epoch 34/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.8127 - acc: 0.7086\n",
      "Epoch 34: val_acc did not improve from 0.69196\n",
      "71/71 [==============================] - 288s 4s/step - loss: 0.8127 - acc: 0.7086 - val_loss: 0.8653 - val_acc: 0.6652\n",
      "Epoch 35/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.8225 - acc: 0.6988\n",
      "Epoch 35: val_acc did not improve from 0.69196\n",
      "71/71 [==============================] - 239s 3s/step - loss: 0.8225 - acc: 0.6988 - val_loss: 1.0371 - val_acc: 0.6071\n",
      "Epoch 36/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.8435 - acc: 0.6952\n",
      "Epoch 36: val_acc improved from 0.69196 to 0.70089, saving model to trained_model.h5\n",
      "71/71 [==============================] - 226s 3s/step - loss: 0.8435 - acc: 0.6952 - val_loss: 0.9280 - val_acc: 0.7009\n",
      "Epoch 37/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.8096 - acc: 0.7041\n",
      "Epoch 37: val_acc did not improve from 0.70089\n",
      "71/71 [==============================] - 238s 3s/step - loss: 0.8096 - acc: 0.7041 - val_loss: 0.9070 - val_acc: 0.6741\n",
      "Epoch 38/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.8329 - acc: 0.6970\n",
      "Epoch 38: val_acc did not improve from 0.70089\n",
      "71/71 [==============================] - 221s 3s/step - loss: 0.8329 - acc: 0.6970 - val_loss: 0.9316 - val_acc: 0.6696\n",
      "Epoch 39/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.7941 - acc: 0.7250\n",
      "Epoch 39: val_acc improved from 0.70089 to 0.70982, saving model to trained_model.h5\n",
      "71/71 [==============================] - 246s 3s/step - loss: 0.7941 - acc: 0.7250 - val_loss: 0.8233 - val_acc: 0.7098\n",
      "Epoch 40/100\n",
      "24/71 [=========>....................] - ETA: 2:52 - loss: 0.7549 - acc: 0.7351"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=100,\n",
    "                              steps_per_epoch=2276//32,\n",
    "                              validation_data=test_generator,\n",
    "                              validation_steps=251//32,\n",
    "                              workers = 4,\n",
    "                              callbacks=[callbacks_list, tensorboard_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab0123",
   "metadata": {},
   "source": [
    "Testing Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f8e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "img_path = 'F:/UWL Study Documents/Dissertation/ds2_test_img.jpg'\n",
    "\n",
    "img = image.load_img(img_path, target_size=(300, 300))\n",
    "img = image.img_to_array(img, dtype=np.uint8)\n",
    "img=np.array(img)/255.0\n",
    "\n",
    "plt.title(\"Loaded Image\")\n",
    "plt.axis('off')\n",
    "plt.imshow(img.squeeze())\n",
    "\n",
    "p=model.predict(img[np.newaxis, ...])\n",
    "\n",
    "#print(\"Predicted shape\",p.shape)\n",
    "print(\"Maximum Probability: \",np.max(p[0], axis=-1))\n",
    "predicted_class = labels[np.argmax(p[0], axis=-1)]\n",
    "print(\"Classified:\",predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d2957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=[]\n",
    "prob=[]\n",
    "print(\"\\n-------------------Individual Probability--------------------------------\\n\")\n",
    "\n",
    "for i,j in enumerate (p[0],0):\n",
    "    print(labels[i].upper(),':',round(j*100,2),'%')\n",
    "    classes.append(labels[i])\n",
    "    prob.append(round(j*100,2))\n",
    "    \n",
    "def plot_bar_x():\n",
    "    # this is for plotting purpose\n",
    "    index = np.arange(len(classes))\n",
    "    plt.bar(index, prob)\n",
    "    plt.xlabel('Labels', fontsize=12)\n",
    "    plt.ylabel('Probability', fontsize=12)\n",
    "    plt.xticks(index, classes, fontsize=12, rotation=20)\n",
    "    plt.title('Probability for loaded image')\n",
    "    plt.show()\n",
    "plot_bar_x()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2f1a2c",
   "metadata": {},
   "source": [
    "Accuracy Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# ________________ Graph 1 -------------------------\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# ________________ Graph 2 -------------------------\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,max(plt.ylim())])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647fb926",
   "metadata": {},
   "source": [
    "Converting to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85b02b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"Garbage.h5\"\n",
    "keras.models.save_model(model, file)\n",
    "\n",
    "# Load the Keras model\n",
    "loaded_model = keras.models.load_model(file)\n",
    "\n",
    "# Convert the Keras model to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model to a file\n",
    "open(\"garbage.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ff25d",
   "metadata": {},
   "source": [
    "# Random Forest and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1093ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained deep learning model (VGG16)\n",
    "base_model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22d800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from an image\n",
    "def extract_features(img_path, model):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = model.predict(x)\n",
    "    return features.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b695443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset and extract features\n",
    "dir_path = 'F:/UWL Study Documents/Dissertation/Dataset3/Garbage classification/Garbage classification'\n",
    "img_list = glob.glob(os.path.join(dir_path, '*/*.jpg'))\n",
    "X = [extract_features(img_path, base_model) for img_path in img_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c3221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract class labels from the image file paths\n",
    "y = [os.path.basename(os.path.dirname(img_path)) for img_path in img_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aad558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe43cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b450cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "rf_predictions = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a3ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Random Forest model\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(classification_report(y_test, rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07dec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors Classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c932a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "knn_predictions = knn_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the KNN model\n",
    "knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
    "print(\"K-Nearest Neighbors Accuracy:\", knn_accuracy)\n",
    "print(classification_report(y_test, knn_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d98b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot a confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, title):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141ebee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain unique class labels from your dataset\n",
    "unique_labels = np.unique(np.concatenate((y_test, knn_predictions, rf_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549bdd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors Confusion Matrix\n",
    "plot_confusion_matrix(y_test, knn_predictions, labels=unique_labels, title='K-Nearest Neighbors Confusion Matrix')\n",
    "\n",
    "# Random Forest Confusion Matrix\n",
    "plot_confusion_matrix(y_test, rf_predictions, labels=unique_labels, title='Random Forest Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0defddb2",
   "metadata": {},
   "source": [
    "Function to test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4662d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test predictions\n",
    "def test_predictions(classifier, model, img_path):\n",
    "    # Extract features from the input image\n",
    "    features = extract_features(img_path, model)\n",
    "\n",
    "    # Make predictions using the classifier\n",
    "    predicted_class = classifier.predict([features])\n",
    "\n",
    "    return predicted_class[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image path to test predictions\n",
    "test_img_path = 'F:/UWL Study Documents/Dissertation/ds2_test_img.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d0552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test K-Nearest Neighbors prediction\n",
    "knn_prediction = test_predictions(knn_classifier, base_model, test_img_path)\n",
    "print(\"K-Nearest Neighbors Prediction:\", knn_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Random Forest prediction\n",
    "rf_prediction = test_predictions(rf_classifier, base_model, test_img_path)\n",
    "print(\"Random Forest Prediction:\", rf_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65f0706",
   "metadata": {},
   "source": [
    "Second Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eafafcf",
   "metadata": {},
   "source": [
    "Testing using cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c7b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "img_path = 'F:/UWL Study Documents/Dissertation/ds2_test_img2.jpg'\n",
    "\n",
    "img = image.load_img(img_path, target_size=(300, 300))\n",
    "img = image.img_to_array(img, dtype=np.uint8)\n",
    "img=np.array(img)/255.0\n",
    "\n",
    "plt.title(\"Loaded Image\")\n",
    "plt.axis('off')\n",
    "plt.imshow(img.squeeze())\n",
    "\n",
    "p=model.predict(img[np.newaxis, ...])\n",
    "\n",
    "#print(\"Predicted shape\",p.shape)\n",
    "print(\"Maximum Probability: \",np.max(p[0], axis=-1))\n",
    "predicted_class = labels[np.argmax(p[0], axis=-1)]\n",
    "print(\"Classified:\",predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3dca8b",
   "metadata": {},
   "source": [
    "Testing using Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada51f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image path to test predictions\n",
    "test_img_path = 'F:/UWL Study Documents/Dissertation/ds2_test_img2.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb1033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test K-Nearest Neighbors prediction\n",
    "knn_prediction = test_predictions(knn_classifier, base_model, test_img_path)\n",
    "print(\"K-Nearest Neighbors Prediction:\", knn_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db32d166",
   "metadata": {},
   "source": [
    "Testing using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d58edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Random Forest prediction\n",
    "rf_prediction = test_predictions(rf_classifier, base_model, test_img_path)\n",
    "print(\"Random Forest Prediction:\", rf_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
